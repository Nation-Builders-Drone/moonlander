{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from collections import deque\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class to hold the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RLGlue:\n",
    "    \"\"\"RLGlue class\n",
    "\n",
    "    args:\n",
    "        env_name (string): the name of the module where the Environment class can be found\n",
    "        agent_name (string): the name of the module where the Agent class can be found\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env_class, agent_class):\n",
    "        self.environment = env_class()\n",
    "        self.agent = agent_class()\n",
    "\n",
    "        self.total_reward = None\n",
    "        self.last_action = None\n",
    "        self.num_steps = None\n",
    "        self.num_episodes = None\n",
    "\n",
    "    def rl_init(self, agent_init_info={}, env_init_info={}):\n",
    "        \"\"\"Initial method called when RLGlue experiment is created\"\"\"\n",
    "        self.environment.env_init(env_init_info)\n",
    "        self.agent.agent_init(agent_init_info)\n",
    "\n",
    "        self.total_reward = 0.0\n",
    "        self.num_steps = 0\n",
    "        self.num_episodes = 0\n",
    "\n",
    "    def rl_start(self, agent_start_info={}, env_start_info={}):\n",
    "        \"\"\"Starts RLGlue experiment\n",
    "\n",
    "        Returns:\n",
    "            tuple: (state, action)\n",
    "        \"\"\"\n",
    "        \n",
    "        self.total_reward = 0.0\n",
    "        self.num_steps = 1\n",
    "\n",
    "        last_state = self.environment.env_start()\n",
    "        self.last_action = self.agent.agent_start(last_state)\n",
    "\n",
    "        observation = (last_state, self.last_action)\n",
    "\n",
    "        return observation\n",
    "\n",
    "    def rl_agent_start(self, observation):\n",
    "        \"\"\"Starts the agent.\n",
    "\n",
    "        Args:\n",
    "            observation: The first observation from the environment\n",
    "\n",
    "        Returns:\n",
    "            The action taken by the agent.\n",
    "        \"\"\"\n",
    "        return self.agent.agent_start(observation)\n",
    "\n",
    "    def rl_agent_step(self, reward, observation):\n",
    "        \"\"\"Step taken by the agent\n",
    "\n",
    "        Args:\n",
    "            reward (float): the last reward the agent received for taking the\n",
    "                last action.\n",
    "            observation : the state observation the agent receives from the\n",
    "                environment.\n",
    "\n",
    "        Returns:\n",
    "            The action taken by the agent.\n",
    "        \"\"\"\n",
    "        return self.agent.agent_step(reward, observation)\n",
    "\n",
    "    def rl_agent_end(self, reward):\n",
    "        \"\"\"Run when the agent terminates\n",
    "\n",
    "        Args:\n",
    "            reward (float): the reward the agent received when terminating\n",
    "        \"\"\"\n",
    "        self.agent.agent_end(reward)\n",
    "\n",
    "    def rl_env_start(self):\n",
    "        \"\"\"Starts RL-Glue environment.\n",
    "\n",
    "        Returns:\n",
    "            (float, state, Boolean): reward, state observation, boolean\n",
    "                indicating termination\n",
    "        \"\"\"\n",
    "        self.total_reward = 0.0\n",
    "        self.num_steps = 1\n",
    "\n",
    "        this_observation = self.environment.env_start()\n",
    "\n",
    "        return this_observation\n",
    "\n",
    "    def rl_env_step(self, action):\n",
    "        \"\"\"Step taken by the environment based on action from agent\n",
    "\n",
    "        Args:\n",
    "            action: Action taken by agent.\n",
    "\n",
    "        Returns:\n",
    "            (float, state, Boolean): reward, state observation, boolean\n",
    "                indicating termination.\n",
    "        \"\"\"\n",
    "        ro = self.environment.env_step(action)\n",
    "        (this_reward, _, terminal) = ro\n",
    "\n",
    "        self.total_reward += this_reward\n",
    "\n",
    "        if terminal:\n",
    "            self.num_episodes += 1\n",
    "        else:\n",
    "            self.num_steps += 1\n",
    "\n",
    "        return ro\n",
    "\n",
    "    def rl_step(self):\n",
    "        \"\"\"Step taken by RLGlue, takes environment step and either step or\n",
    "            end by agent.\n",
    "\n",
    "        Returns:\n",
    "            (float, state, action, Boolean): reward, last state observation,\n",
    "                last action, boolean indicating termination\n",
    "        \"\"\"\n",
    "\n",
    "        (reward, last_state, term) = self.environment.env_step(self.last_action)\n",
    "\n",
    "        self.total_reward += reward;\n",
    "\n",
    "        if term:\n",
    "            self.num_episodes += 1\n",
    "            self.agent.agent_end(reward)\n",
    "            roat = (reward, last_state, None, term)\n",
    "        else:\n",
    "            self.num_steps += 1\n",
    "            self.last_action = self.agent.agent_step(reward, last_state)\n",
    "            roat = (reward, last_state, self.last_action, term)\n",
    "\n",
    "        return roat\n",
    "\n",
    "    def rl_cleanup(self):\n",
    "        \"\"\"Cleanup done at end of experiment.\"\"\"\n",
    "        self.environment.env_cleanup()\n",
    "        self.agent.agent_cleanup()\n",
    "\n",
    "    def rl_agent_message(self, message):\n",
    "        \"\"\"Message passed to communicate with agent during experiment\n",
    "\n",
    "        Args:\n",
    "            message: the message (or question) to send to the agent\n",
    "\n",
    "        Returns:\n",
    "            The message back (or answer) from the agent\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        return self.agent.agent_message(message)\n",
    "\n",
    "    def rl_env_message(self, message):\n",
    "        \"\"\"Message passed to communicate with environment during experiment\n",
    "\n",
    "        Args:\n",
    "            message: the message (or question) to send to the environment\n",
    "\n",
    "        Returns:\n",
    "            The message back (or answer) from the environment\n",
    "\n",
    "        \"\"\"\n",
    "        return self.environment.env_message(message)\n",
    "\n",
    "    def rl_episode(self, max_steps_this_episode):\n",
    "        \"\"\"Runs an RLGlue episode\n",
    "\n",
    "        Args:\n",
    "            max_steps_this_episode (Int): the maximum steps for the experiment to run in an episode\n",
    "\n",
    "        Returns:\n",
    "            Boolean: if the episode should terminate\n",
    "        \"\"\"\n",
    "        is_terminal = False\n",
    "\n",
    "        self.rl_start()\n",
    "\n",
    "        while (not is_terminal) and ((max_steps_this_episode == 0) or\n",
    "                                     (self.num_steps < max_steps_this_episode)):\n",
    "            rl_step_result = self.rl_step()\n",
    "            is_terminal = rl_step_result[3]\n",
    "\n",
    "        return is_terminal\n",
    "\n",
    "    def rl_return(self):\n",
    "        \"\"\"The total reward\n",
    "\n",
    "        Returns:\n",
    "            float: the total reward\n",
    "        \"\"\"\n",
    "        return self.total_reward\n",
    "\n",
    "    def rl_num_steps(self):\n",
    "        \"\"\"The total number of steps taken\n",
    "\n",
    "        Returns:\n",
    "            Int: the total number of steps taken\n",
    "        \"\"\"\n",
    "        return self.num_steps\n",
    "\n",
    "    def rl_num_episodes(self):\n",
    "        \"\"\"The number of episodes\n",
    "\n",
    "        Returns\n",
    "            Int: the total number of episodes\n",
    "\n",
    "        \"\"\"\n",
    "        return self.num_episodes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the Lunar Lander environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LunarLanderEnvironment():\n",
    "    def env_init(self, env_info={}):\n",
    "        \"\"\"\n",
    "        Setup for the environment called when the experiment first starts.\n",
    "        \"\"\"\n",
    "        self.env = gym.make(\"LunarLander-v2\")\n",
    "        self.env.seed(0)\n",
    "\n",
    "    def env_start(self):\n",
    "        \"\"\"\n",
    "        The first method called when the experiment starts, called before the\n",
    "        agent starts.\n",
    "\n",
    "        Returns:\n",
    "            The first state observation from the environment.\n",
    "        \"\"\"        \n",
    "        \n",
    "        reward = 0.0\n",
    "        observation = self.env.reset()\n",
    "        is_terminal = False\n",
    "                \n",
    "        self.reward_obs_term = (reward, observation, is_terminal)\n",
    "        \n",
    "        # return first state observation from the environment\n",
    "        return self.reward_obs_term[1]\n",
    "        \n",
    "    def env_step(self, action):\n",
    "        \"\"\"A step taken by the environment.\n",
    "\n",
    "        Args:\n",
    "            action: The action taken by the agent\n",
    "\n",
    "        Returns:\n",
    "            (float, state, Boolean): a tuple of the reward, state observation,\n",
    "                and boolean indicating if it's terminal.\n",
    "        \"\"\"\n",
    "\n",
    "        last_state = self.reward_obs_term[1]\n",
    "        current_state, reward, is_terminal, _ = self.env.step(action)\n",
    "        \n",
    "        self.reward_obs_term = (reward, current_state, is_terminal)\n",
    "        \n",
    "        return self.reward_obs_term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a simple Neural network with 1 hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActionValueNetwork:\n",
    "    def __init__(self, network_config):\n",
    "        self.state_dim = network_config.get(\"state_dim\")\n",
    "        self.num_hidden_units = network_config.get(\"num_hidden_units\")\n",
    "        self.num_actions = network_config.get(\"num_actions\")\n",
    "\n",
    "        self.rand_generator = np.random.RandomState(network_config.get(\"seed\"))\n",
    "\n",
    "        self.layer_sizes = [self.state_dim, self.num_hidden_units, self.num_actions]\n",
    "\n",
    "        # Initialize the weights of the neural network\n",
    "        self.weights = [dict() for i in range(0, len(self.layer_sizes) - 1)]\n",
    "        for i in range(0, len(self.layer_sizes) - 1):\n",
    "            self.weights[i]['W'] = self.init_saxe(self.layer_sizes[i], self.layer_sizes[i + 1])\n",
    "            self.weights[i]['b'] = np.zeros((1, self.layer_sizes[i + 1]))\n",
    "\n",
    "    def get_action_values(self, s):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            s (Numpy array): The state.\n",
    "        Returns:\n",
    "            The action-values (Numpy array) calculated using the network's weights.\n",
    "        \"\"\"\n",
    "\n",
    "        W0, b0 = self.weights[0]['W'], self.weights[0]['b']\n",
    "        psi = np.dot(s, W0) + b0\n",
    "        x = np.maximum(psi, 0)\n",
    "        self.x = x\n",
    "\n",
    "        W1, b1 = self.weights[1]['W'], self.weights[1]['b']\n",
    "        q_vals = np.dot(x, W1) + b1\n",
    "\n",
    "        return q_vals\n",
    "\n",
    "    def get_TD_update(self, s, delta_mat):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            s (Numpy array): The state.\n",
    "            delta_mat (Numpy array): A 2D array of shape (batch_size, num_actions). Each row of delta_mat\n",
    "            correspond to one state in the batch. Each row has only one non-zero element\n",
    "            which is the TD-error corresponding to the action taken.\n",
    "        Returns:\n",
    "            The TD update (Array of dictionaries with gradient times TD errors) for the network's weights\n",
    "        \"\"\"\n",
    "\n",
    "        x = self.x\n",
    "        \n",
    "        td_update = [dict() for i in range(len(self.weights))]\n",
    "\n",
    "        v1 = delta_mat\n",
    "        td_update[1]['W'] = np.dot(x.T, v1) / s.shape[0]\n",
    "        td_update[1]['b'] = np.sum(v1, axis=0, keepdims=True) / s.shape[0]\n",
    "        \n",
    "        W1, b1 = self.weights[1]['W'], self.weights[1]['b']\n",
    "        \n",
    "        v2 = np.dot(v1, W1.T) * (x > 0).astype(float)\n",
    "        td_update[0]['W'] = np.dot(s.T, v2) / s.shape[0]\n",
    "        td_update[0]['b'] = np.sum(v2, axis=0, keepdims=True) / s.shape[0]\n",
    "\n",
    "        return td_update\n",
    "\n",
    "    # You may wish to read the relevant paper for more information on this weight initialization\n",
    "    # (Exact solutions to the nonlinear dynamics of learning in deep linear neural networks by Saxe, A et al., 2013)\n",
    "    def init_saxe(self, rows, cols):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            rows (int): number of input units for layer.\n",
    "            cols (int): number of output units for layer.\n",
    "        Returns:\n",
    "            NumPy Array consisting of weights for the layer based on the initialization in Saxe et al.\n",
    "        \"\"\"\n",
    "        tensor = self.rand_generator.normal(0, 1, (rows, cols))\n",
    "        if rows < cols:\n",
    "            tensor = tensor.T\n",
    "        tensor, r = np.linalg.qr(tensor)\n",
    "        d = np.diag(r, 0)\n",
    "        ph = np.sign(d)\n",
    "        tensor *= ph\n",
    "\n",
    "        if rows < cols:\n",
    "            tensor = tensor.T\n",
    "        return tensor\n",
    "\n",
    "    def get_weights(self):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            A copy of the current weights of this network.\n",
    "        \"\"\"\n",
    "        return deepcopy(self.weights)\n",
    "\n",
    "    def set_weights(self, weights):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            weights (list of dictionaries): Consists of weights that this network will set as its own weights.\n",
    "        \"\"\"\n",
    "        self.weights = deepcopy(weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the Neural net Optimiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adam():\n",
    "    def __init__(self, layer_sizes, \n",
    "                 optimizer_info):\n",
    "        self.layer_sizes = layer_sizes\n",
    "\n",
    "        # Specify Adam algorithm's hyper parameters\n",
    "        self.step_size = optimizer_info.get(\"step_size\")\n",
    "        self.beta_m = optimizer_info.get(\"beta_m\")\n",
    "        self.beta_v = optimizer_info.get(\"beta_v\")\n",
    "        self.epsilon = optimizer_info.get(\"epsilon\")\n",
    "        \n",
    "        # Initialize Adam algorithm's m and v\n",
    "        self.m = [dict() for i in range(1, len(self.layer_sizes))]\n",
    "        self.v = [dict() for i in range(1, len(self.layer_sizes))]\n",
    "        \n",
    "        for i in range(0, len(self.layer_sizes) - 1):\n",
    "            self.m[i][\"W\"] = np.zeros((layer_sizes[i], layer_sizes[i + 1]))\n",
    "            self.m[i][\"b\"] = np.zeros((1, layer_sizes[i + 1]))\n",
    "            self.v[i][\"W\"] = np.zeros((layer_sizes[i], layer_sizes[i + 1]))\n",
    "            self.v[i][\"b\"] = np.zeros((1, layer_sizes[i + 1]))\n",
    "            \n",
    "        self.beta_m_product = self.beta_m\n",
    "        self.beta_v_product = self.beta_v    \n",
    "        \n",
    "    def update_weights(self, weights, td_errors_times_gradients):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            weights (Array of dictionaries): The weights of the neural network.\n",
    "            td_errors_times_gradients (Array of dictionaries): The gradient of the \n",
    "            action-values with respect to the network's weights times the TD-error\n",
    "        Returns:\n",
    "            The updated weights (Array of dictionaries).\n",
    "        \"\"\"\n",
    "        for i in range(len(weights)):\n",
    "            for param in weights[i].keys():\n",
    "                self.m[i][param] = self.beta_m * self.m[i][param] + (1.0 - self.beta_m) * td_errors_times_gradients[i][param]\n",
    "                self.v[i][param] = self.beta_v * self.v[i][param] + (1.0 - self.beta_v) * (td_errors_times_gradients[i][param] * td_errors_times_gradients[i][param])\n",
    "                m_hat = self.m[i][param] / (1.0 - self.beta_m_product)\n",
    "                v_hat = self.v[i][param] / (1.0 - self.beta_v_product)\n",
    "                weight_update = self.step_size / (np.sqrt(v_hat) + self.epsilon) * m_hat\n",
    "                \n",
    "                weights[i][param] = weights[i][param] + weight_update\n",
    "\n",
    "        self.beta_m_product *= self.beta_m\n",
    "        self.beta_v_product *= self.beta_v\n",
    "        \n",
    "        return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the replay buffer to simulate a model for faster learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, size, minibatch_size, seed):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            size (integer): The size of the replay buffer.              \n",
    "            minibatch_size (integer): The sample size.\n",
    "            seed (integer): The seed for the random number generator. \n",
    "        \"\"\"\n",
    "        self.buffer = []\n",
    "        self.minibatch_size = minibatch_size\n",
    "        self.rand_generator = np.random.RandomState(seed)\n",
    "        self.max_size = size\n",
    "\n",
    "    def append(self, state, action, reward, terminal, next_state):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            state (Numpy array): The state.              \n",
    "            action (integer): The action.\n",
    "            reward (float): The reward.\n",
    "            terminal (integer): 1 if the next state is a terminal state and 0 otherwise.\n",
    "            next_state (Numpy array): The next state.           \n",
    "        \"\"\"\n",
    "        if len(self.buffer) == self.max_size:\n",
    "            del self.buffer[0]\n",
    "        self.buffer.append([state, action, reward, terminal, next_state])\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            A list of transition tuples including state, action, reward, terinal, and next_state\n",
    "        \"\"\"\n",
    "        idxs = self.rand_generator.choice(np.arange(len(self.buffer)), size=self.minibatch_size)\n",
    "        return [self.buffer[idx] for idx in idxs]\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.buffer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the action probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(action_values, tau=1.0):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        action_values (Numpy array): A 2D array of shape (batch_size, num_actions). \n",
    "                       The action-values computed by an action-value network.              \n",
    "        tau (float): The temperature parameter scalar.\n",
    "    Returns:\n",
    "        A 2D array of shape (batch_size, num_actions). Where each column is a probability distribution over\n",
    "        the actions representing the policy.\n",
    "    \"\"\"\n",
    "    preferences = action_values / tau\n",
    "    max_preference = np.max(preferences, 1)\n",
    "        \n",
    "    reshaped_max_preference = max_preference.reshape((-1, 1))\n",
    "    \n",
    "    exp_preferences = np.exp(preferences - reshaped_max_preference)\n",
    "    sum_of_exp_preferences = np.sum(exp_preferences, 1)\n",
    "    \n",
    "    reshaped_sum_of_exp_preferences = sum_of_exp_preferences.reshape((-1, 1))\n",
    "    action_probs = exp_preferences / reshaped_sum_of_exp_preferences\n",
    "    \n",
    "    action_probs = action_probs.squeeze()\n",
    "    \n",
    "    return action_probs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the Temporal Difference error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_td_error(states, next_states, actions, rewards, discount, terminals, network, current_q, tau):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        states (Numpy array): The batch of states with the shape (batch_size, state_dim).\n",
    "        next_states (Numpy array): The batch of next states with the shape (batch_size, state_dim).\n",
    "        actions (Numpy array): The batch of actions with the shape (batch_size,).\n",
    "        rewards (Numpy array): The batch of rewards with the shape (batch_size,).\n",
    "        discount (float): The discount factor.\n",
    "        terminals (Numpy array): The batch of terminals with the shape (batch_size,).\n",
    "        network (ActionValueNetwork): The latest state of the network that is getting replay updates.\n",
    "        current_q (ActionValueNetwork): The fixed network used for computing the targets,\n",
    "                                        and particularly, the action-values at the next-states.\n",
    "    Returns:\n",
    "        The TD errors (Numpy array) for actions taken, of shape (batch_size,)\n",
    "    \"\"\"\n",
    "\n",
    "    # Note: Here network is the latest state of the network that is getting replay updates. In other words,\n",
    "    # the network represents Q_{t+1}^{i} whereas current_q represents Q_t, the fixed network used for computing the\n",
    "    # targets, and particularly, the action-values at the next-states.\n",
    "\n",
    "    # Compute action values at next states using current_q network\n",
    "    q_next_mat = current_q.get_action_values(next_states)\n",
    "\n",
    "    # Compute policy at next state by passing the action-values in q_next_mat to softmax()\n",
    "    probs_mat = softmax(q_next_mat, tau)\n",
    "\n",
    "    # Compute the estimate of the next state value, v_next_vec.\n",
    "    v_next_vec = np.sum(q_next_mat * probs_mat, 1) * (1.0 - terminals)\n",
    "\n",
    "    # Compute Expected Sarsa target\n",
    "    target_vec = rewards + discount * v_next_vec\n",
    "\n",
    "    # Compute action values at the current states for all actions using network\n",
    "    q_mat = network.get_action_values(states)\n",
    "\n",
    "    # Batch Indices is an array from 0 to the batch size - 1.\n",
    "    batch_indices = np.arange(q_mat.shape[0])\n",
    "\n",
    "    # Compute q_vec by selecting q(s, a) from q_mat for taken actions\n",
    "    q_vec = q_mat[batch_indices, actions]\n",
    "\n",
    "    # Compute TD errors for actions taken\n",
    "    delta_vec = target_vec - q_vec\n",
    "\n",
    "    return delta_vec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the network optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_network(experiences, discount, optimizer, network, current_q, tau):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        experiences (Numpy array): The batch of experiences including the states, actions,\n",
    "                                   rewards, terminals, and next_states.\n",
    "        discount (float): The discount factor.\n",
    "        network (ActionValueNetwork): The latest state of the network that is getting replay updates.\n",
    "        current_q (ActionValueNetwork): The fixed network used for computing the targets,\n",
    "                                        and particularly, the action-values at the next-states.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get states, action, rewards, terminals, and next_states from experiences\n",
    "    states, actions, rewards, terminals, next_states = map(list, zip(*experiences))\n",
    "    states = np.concatenate(states)\n",
    "    next_states = np.concatenate(next_states)\n",
    "    rewards = np.array(rewards)\n",
    "    terminals = np.array(terminals)\n",
    "    batch_size = states.shape[0]\n",
    "\n",
    "    # Compute TD error using the get_td_error function\n",
    "    delta_vec = get_td_error(states, next_states, actions, rewards, discount, terminals, network, current_q, tau)\n",
    "\n",
    "    # Batch Indices is an array from 0 to the batch_size - 1.\n",
    "    batch_indices = np.arange(batch_size)\n",
    "\n",
    "    # Make a td error matrix of shape (batch_size, num_actions)\n",
    "    # delta_mat has non-zero value only for actions taken\n",
    "    delta_mat = np.zeros((batch_size, network.num_actions))\n",
    "    delta_mat[batch_indices, actions] = delta_vec\n",
    "\n",
    "    # Pass delta_mat to compute the TD errors times the gradients of the network's weights from back-propagation\n",
    "    td_update = network.get_TD_update(states, delta_mat)\n",
    "\n",
    "    # Pass network.get_weights and the td_update to the optimizer to get updated weights\n",
    "    weights = optimizer.update_weights(network.get_weights(), td_update)\n",
    "\n",
    "    network.set_weights(weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self):\n",
    "        self.name = \"expected_sarsa_agent\"\n",
    "        \n",
    "    def agent_init(self, agent_config):\n",
    "        \"\"\"Setup for the agent called when the experiment first starts.\n",
    "\n",
    "        Set parameters needed to setup the agent.\n",
    "\n",
    "        Assume agent_config dict contains:\n",
    "        {\n",
    "            network_config: dictionary,\n",
    "            optimizer_config: dictionary,\n",
    "            replay_buffer_size: integer,\n",
    "            minibatch_sz: integer, \n",
    "            num_replay_updates_per_step: float\n",
    "            discount_factor: float,\n",
    "        }\n",
    "        \"\"\"\n",
    "        self.replay_buffer = ReplayBuffer(agent_config['replay_buffer_size'], \n",
    "                                          agent_config['minibatch_sz'], agent_config.get(\"seed\"))\n",
    "        self.network = ActionValueNetwork(agent_config['network_config'])\n",
    "        self.optimizer = Adam(self.network.layer_sizes, agent_config[\"optimizer_config\"])\n",
    "        self.num_actions = agent_config['network_config']['num_actions']\n",
    "        self.num_replay = agent_config['num_replay_updates_per_step']\n",
    "        self.discount = agent_config['gamma']\n",
    "        self.tau = agent_config['tau']\n",
    "        \n",
    "        self.rand_generator = np.random.RandomState(agent_config.get(\"seed\"))\n",
    "        \n",
    "        self.last_state = None\n",
    "        self.last_action = None\n",
    "        \n",
    "        self.sum_rewards = 0\n",
    "        self.episode_steps = 0\n",
    "\n",
    "    def policy(self, state):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            state (Numpy array): the state.\n",
    "        Returns:\n",
    "            the action. \n",
    "        \"\"\"\n",
    "        action_values = self.network.get_action_values(state)\n",
    "        probs_batch = softmax(action_values, self.tau)\n",
    "        action = self.rand_generator.choice(self.num_actions, p=probs_batch.squeeze())\n",
    "        \n",
    "        return action\n",
    "\n",
    "    def agent_start(self, state):\n",
    "        \"\"\"The first method called when the experiment starts, called after\n",
    "        the environment starts.\n",
    "        Args:\n",
    "            state (Numpy array): the state from the\n",
    "                environment's evn_start function.\n",
    "        Returns:\n",
    "            The first action the agent takes.\n",
    "        \"\"\"\n",
    "        self.sum_rewards = 0\n",
    "        self.episode_steps = 0\n",
    "        self.last_state = np.array([state])\n",
    "        self.last_action = self.policy(self.last_state)\n",
    "        \n",
    "        return self.last_action\n",
    "\n",
    "    # weights update using optimize_network, and updating last_state and last_action (~5 lines).\n",
    "    def agent_step(self, reward, state):\n",
    "        \"\"\"A step taken by the agent.\n",
    "        Args:\n",
    "            reward (float): the reward received for taking the last action taken\n",
    "            state (Numpy array): the state from the\n",
    "                environment's step based, where the agent ended up after the\n",
    "                last step\n",
    "        Returns:\n",
    "            The action the agent is taking.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.sum_rewards += reward\n",
    "        self.episode_steps += 1\n",
    "\n",
    "        # Make state an array of shape (1, state_dim) to add a batch dimension and\n",
    "        # to later match the get_action_values() and get_TD_update() functions\n",
    "        state = np.array([state])\n",
    "\n",
    "        # Select action\n",
    "        action = self.policy(state)\n",
    "        \n",
    "        # Append new experience to replay buffer\n",
    "        self.replay_buffer.append(self.last_state, self.last_action, reward, 0, state)\n",
    "        \n",
    "        # Perform replay steps:\n",
    "        if self.replay_buffer.size() > self.replay_buffer.minibatch_size:\n",
    "            current_q = deepcopy(self.network)\n",
    "            for _ in range(self.num_replay):\n",
    "                # Get sample experiences from the replay buffer\n",
    "                experiences = self.replay_buffer.sample()\n",
    "                \n",
    "                # Call optimize_network to update the weights of the network (~1 Line)\n",
    "                optimize_network(experiences, self.discount, self.optimizer, self.network, current_q, self.tau)\n",
    "                \n",
    "        # Update the last state and last action.\n",
    "        self.last_state = state\n",
    "        self.last_action = action\n",
    "        \n",
    "        return action\n",
    "\n",
    "    # update of the weights using optimize_network (~2 lines).\n",
    "    def agent_end(self, reward):\n",
    "        \"\"\"Run when the agent terminates.\n",
    "        Args:\n",
    "            reward (float): the reward the agent received for entering the\n",
    "                terminal state.\n",
    "        \"\"\"\n",
    "        self.sum_rewards += reward\n",
    "        self.episode_steps += 1\n",
    "        \n",
    "        # Set terminal state to an array of zeros\n",
    "        state = np.zeros_like(self.last_state)\n",
    "\n",
    "        # Append new experience to replay buffer\n",
    "        self.replay_buffer.append(self.last_state, self.last_action, reward, 1, state)\n",
    "        \n",
    "        # Perform replay steps:\n",
    "        if self.replay_buffer.size() > self.replay_buffer.minibatch_size:\n",
    "            current_q = deepcopy(self.network)\n",
    "            for _ in range(self.num_replay):\n",
    "                # Get sample experiences from the replay buffer\n",
    "                experiences = self.replay_buffer.sample()\n",
    "                \n",
    "                # Call optimize_network to update the weights of the network\n",
    "                optimize_network(experiences, self.discount, self.optimizer, self.network, current_q, self.tau)\n",
    "        \n",
    "    def agent_message(self, message):\n",
    "        if message == \"get_sum_reward\":\n",
    "            return self.sum_rewards\n",
    "        else:\n",
    "            raise Exception(\"Unrecognized Message!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the experiment function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(environment, agent, environment_parameters, agent_parameters, experiment_parameters):\n",
    "    \n",
    "    rl_glue = RLGlue(environment, agent)\n",
    "        \n",
    "    # save sum of reward at the end of each episode\n",
    "    agent_sum_reward = np.zeros((experiment_parameters[\"num_runs\"], \n",
    "                                 experiment_parameters[\"num_episodes\"]))\n",
    "\n",
    "    env_info = {}\n",
    "\n",
    "    agent_info = agent_parameters\n",
    "\n",
    "    # one agent setting\n",
    "    for run in range(1, experiment_parameters[\"num_runs\"]+1):\n",
    "        agent_info[\"seed\"] = run\n",
    "        agent_info[\"network_config\"][\"seed\"] = run\n",
    "        env_info[\"seed\"] = run\n",
    "\n",
    "        rl_glue.rl_init(agent_info, env_info)\n",
    "        \n",
    "        for episode in tqdm(range(1, experiment_parameters[\"num_episodes\"]+1)):\n",
    "            # run episode\n",
    "            rl_glue.rl_episode(experiment_parameters[\"timeout\"])\n",
    "            \n",
    "            episode_reward = rl_glue.rl_agent_message(\"get_sum_reward\")\n",
    "            agent_sum_reward[run - 1, episode - 1] = episode_reward\n",
    "\n",
    "    save_name = \"{}\".format(rl_glue.agent.name)\n",
    "    if not os.path.exists('results'):\n",
    "        os.makedirs('results')\n",
    "    np.save(\"results/sum_reward_{}\".format(save_name), agent_sum_reward)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_legend_dict = {\"expected_sarsa_agent\": \"Expected SARSA with neural network\", \"random_agent\": \"Random\"}\n",
    "path_dict = {\"expected_sarsa_agent\": \"results/\", \"random_agent\": \"./\"}\n",
    "\n",
    "plt_label_dict = {\"expected_sarsa_agent\": \"Sum of\\nreward\\nduring\\nepisode\"}\n",
    "\n",
    "def smooth(data, k):\n",
    "    num_episodes = data.shape[1]\n",
    "    num_runs = data.shape[0]\n",
    "\n",
    "    smoothed_data = np.zeros((num_runs, num_episodes))\n",
    "\n",
    "    for i in range(num_episodes):\n",
    "        if i < k:\n",
    "            smoothed_data[:, i] = np.mean(data[:, :i+1], axis = 1)   \n",
    "        else:\n",
    "            smoothed_data[:, i] = np.mean(data[:, i-k:i+1], axis = 1)    \n",
    "        \n",
    "\n",
    "    return smoothed_data\n",
    "\n",
    "# Function to plot result\n",
    "def plot_result(data_name_array):\n",
    "    plt_agent_sweeps = []\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8,6))\n",
    "    \n",
    "    for data_name in data_name_array:\n",
    "        \n",
    "        # load data\n",
    "        filename = 'sum_reward_{}'.format(data_name).replace('.','')\n",
    "        sum_reward_data = np.load('{}/{}.npy'.format(path_dict[data_name], filename))\n",
    "\n",
    "        # smooth data\n",
    "        smoothed_sum_reward = smooth(data = sum_reward_data, k = 100)\n",
    "        \n",
    "        mean_smoothed_sum_reward = np.mean(smoothed_sum_reward, axis = 0)\n",
    "\n",
    "        plot_x_range = np.arange(0, mean_smoothed_sum_reward.shape[0])\n",
    "        graph_current_agent_sum_reward, = ax.plot(plot_x_range, mean_smoothed_sum_reward[:], label=plt_legend_dict[data_name])\n",
    "        plt_agent_sweeps.append(graph_current_agent_sum_reward)\n",
    "    \n",
    "    ax.legend(handles=plt_agent_sweeps, fontsize = 13)\n",
    "    ax.set_title(\"Learning Curve\", fontsize = 15)\n",
    "    ax.set_xlabel('Episodes', fontsize = 14)\n",
    "    ax.set_ylabel(plt_label_dict[data_name_array[0]], rotation=0, labelpad=40, fontsize = 14)\n",
    "    ax.set_ylim([-300, 300])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the actual experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2020-05-28 05:42:05,439] Making new env: LunarLander-v2\n",
      "/big1/installs/anaconda3/lib/python3.7/site-packages/gym/envs/registration.py:17: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n",
      "100%|██████████| 600/600 [25:54<00:00,  2.59s/it]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "# parameters\n",
    "experiment_parameters = {\n",
    "    \"num_runs\" : 1,\n",
    "    \"num_episodes\" : 600,\n",
    "    # OpenAI Gym environments allow for a timestep limit timeout, causing episodes to end after some number of timesteps. Here we use the default of 1000.\n",
    "    \"timeout\" : 1000\n",
    "}\n",
    "\n",
    "# Environment parameters\n",
    "environment_parameters = {}\n",
    "\n",
    "current_env = LunarLanderEnvironment\n",
    "\n",
    "# Agent parameters\n",
    "agent_parameters = {\n",
    "    'network_config': {\n",
    "        'state_dim': 8,\n",
    "        'num_hidden_units': 1024,\n",
    "        'num_actions': 4\n",
    "    },\n",
    "    'optimizer_config': {\n",
    "        'step_size': 1e-3,\n",
    "        'beta_m': 0.9, \n",
    "        'beta_v': 0.999,\n",
    "        'epsilon': 1e-8\n",
    "    },\n",
    "    'replay_buffer_size': 50000,\n",
    "    'minibatch_sz': 8,\n",
    "    'num_replay_updates_per_step': 4,\n",
    "    'gamma': 0.99,\n",
    "    'tau': 0.001\n",
    "}\n",
    "current_agent = Agent\n",
    "\n",
    "# run experiment\n",
    "run_experiment(current_env, current_agent, environment_parameters, agent_parameters, experiment_parameters)\n",
    "\n",
    "plot_result([\"expected_sarsa_agent\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div align=\"middle\">\n",
       "<video width=\"80%\" controls>\n",
       "  <source src=\"MoonLander.mp4\" type=\"video/mp4\">\n",
       "</video></div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<div align=\"middle\">\n",
    "<video width=\"80%\" controls>\n",
    "  <source src=\"MoonLander.mp4\" type=\"video/mp4\">\n",
    "</video></div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
